{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+eN6fRVeB424TE4coLevZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rchejfec/IRPP-oasis-llm-automation-ratings-guide/blob/main/HarnessingAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 - Set up and data prep\n",
        "\n",
        "The easiest way to get started, especially if you're new to python or LLMs, is to use Google Colab. Its free and relatively easy to use. Just log in with or make a new google account, open this file, and upload the datasets included here. You can also use your google drive and there are many tutorials online if you get stuck.\n",
        "\n",
        "If you don't use Colab, or a similar developer environment, you'll have to manage your own dependencies, versions, and keys, which can be a bit daunting if you're not used to it but its not much more complicated.  "
      ],
      "metadata": {
        "id": "VnVnfEwyuQp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sWCD4jIiSD76"
      },
      "outputs": [],
      "source": [
        "# If you're using Google Colab, most likely you only need to install DSPy\n",
        "\n",
        "!pip install dspy-ai -q\n",
        "\n",
        "# if you're not using Google Colab, ensure the following are installed.\n",
        "# !pip google-generativeai pandas json #etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json"
      ],
      "metadata": {
        "id": "6ipyXAAZSVIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll first get a list of all unique skills and work activities in ESDC's OaSIS, wich you can download from this repo or here. If you wanted to replace it with some other source (O*Net's for example), just ensure the final list is in a dict and follows the structure of `items_to_rate`."
      ],
      "metadata": {
        "id": "xtAQKG_-YenW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the working directory so that you can use relative paths.\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Projects/Harnessing AI\") # update with your own\n",
        "\n",
        "try:\n",
        "  # OaSIS Guide:\n",
        "  guide_df = pd.read_csv(\"Data/OaSIS_Guide_2023.csv\")\n",
        "\n",
        "  # Extract skills & work activities\n",
        "  items_to_rate_df = guide_df[(guide_df['Structure type'] == \"Descriptor\") &\n",
        "                          (guide_df['Category'].isin([\"Skills\", \"Work Activities\"]))]\n",
        "\n",
        "  # Convert to dict for easier wrangling\n",
        "  items_to_rate = items_to_rate_df[['Name', 'Description']].to_dict(orient=\"records\")\n",
        "\n",
        "  # Remove unecessary information that might interfere with prompts.\n",
        "  pattern_to_remove = r\"\\s*This descriptor is measured by .*? level on a scale of \\d+-\\d+\\.?\\s*$\"\n",
        "\n",
        "  for item in items_to_rate:\n",
        "    original = item.get(\"Description\")\n",
        "    cleaned = re.sub(pattern_to_remove, \"\", original, flags=re.IGNORECASE).strip()\n",
        "    item[\"Description\"] = cleaned\n",
        "\n",
        "  print(f\"Extracted dictionary with {len(items_to_rate)} items with {list(items_to_rate[0].keys())} as keys.\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Failed to make a dictionary from input table. Ensure you have uploaded the file and that file names are aligned.\\\n",
        "  \\nError message: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BWmP_pP-esP5",
        "outputId": "51a146a7-1293-4f5e-ff38-98c5cac35db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted dictionary with 99 items with ['Name', 'Description'] as keys.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll import a list of different phrasings to use when prompting the LLM. These come from Appendix B of the study, with some slight modificiations. I omit versions 7 and 9, since I couldn't find an easy way to handle changing those parameters on the fly across different model providers. I also incorporate the name and (sometimes) description of the skills and work activities into the prompt using tags (i.e `{item_name}`). The goal here was to make it easier for the LLM to parse the question, as some models are better than others at processing longer, formally structured requests.\n",
        "\n",
        "You can download the prompts from `data/prompt_templates.json`. Edit it using a text editor to remove, modify, or add new phrasings. For example, while only some of the original study's prompts included a description of the skill/activity, you could ensure all of them do by adding the tag `{description}` where it makes sense."
      ],
      "metadata": {
        "id": "k-V-Ueqt2BnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  with open(\"Data/prompt_templates.json\", \"r\") as f:\n",
        "    prompts = json.load(f)\n",
        "    print(f\"Imported {len(prompts)} prompts.\")\n",
        "except Exception as e:\n",
        "  print(f\"Failed to import list of prompts. Ensure you have uploaded the file and that file names are aligned.\\\n",
        "  \\nError message: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XDlMViM4mn4f",
        "outputId": "5cd5fb53-b461-41f3-e7b8-87f704ddac36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported 10 prompts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 - Configure model via DSPy\n",
        "\n",
        "The next step is to initialize and configure our model and define the structure of our prompts and expected response.\n",
        "\n",
        "I used Google's Gemma-3-1b-it and got my API key through Google Studio. You could just as easily use ChatGPT by finding the exact name of the model (should look like `basemodel/model-version`) and getting yourself an API key. The same could be said of a Llama model using Groq for example, or virtually any other major model available.\n",
        "\n",
        "It is important that you don't share or accidentally publish your API key. That's why the code below uses Google Colab's *secrets* feature, which makes things pretty easy. Just click on the key icon on the left navigation bar, add your key and give it a name. Remember to update `API_KEY_NAME` below too. If you rather load your key locally, you can check this out for some resources."
      ],
      "metadata": {
        "id": "knrOJaVJ6nzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the name and version of the LLM (Defaults to Gemma)\n",
        "LLM_PROVIDER_MODEL_STRING = \"gemini/gemma-3-1b-it\"\n",
        "\n",
        "# Initialize variables\n",
        "API_KEY = None\n",
        "llm = None\n",
        "API_KEY_NAME = \"GOOGLE_API_KEY\"   # update with your own name if different\n",
        "\n",
        "# Load API Key\n",
        "# If running in Google Colab, set your API key in the \"Secrets\" tab.\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "  API_KEY = userdata.get(API_KEY_NAME)\n",
        "  if API_KEY:\n",
        "      print(f\"{API_KEY_NAME} loaded from Colab secrets.\")\n",
        "except ImportError:\n",
        "  print(\"Could not import Colab userdata\")\n",
        "  pass\n",
        "\n",
        "# If not using Google Colab, make sure to implement your key loading here\n",
        "# #######\n",
        "# #######\n",
        "\n",
        "# Set model parameters\n",
        "if API_KEY:\n",
        "  try:\n",
        "      model_config = {\n",
        "          \"temperature\": 0.1,     # change the desired temperature\n",
        "          \"max_tokens\": 300}      # change the desired number of max tokens\n",
        "      llm = dspy.LM(\n",
        "          model=LLM_PROVIDER_MODEL_STRING,\n",
        "          api_key=API_KEY,\n",
        "          temperature=model_config[\"temperature\"],\n",
        "          max_tokens= model_config[\"max_tokens\"],\n",
        "          timeout=60,             # number of seconds before it retries. lengthen if your connection is spotty\n",
        "          num_retries=3           # number of times it should retry after timing out\n",
        "          )\n",
        "\n",
        "      print(f\"DSPy configured to use model: {LLM_PROVIDER_MODEL_STRING}\")\n",
        "  except Exception as e:\n",
        "      print(f\"ERROR: Could not configure DSPy with {LLM_PROVIDER_MODEL_STRING}. Exception: {e}\")\n",
        "      llm = None\n",
        "else:\n",
        "  print(f\"ERROR: {API_KEY_NAME} not found. Please set it in Colab secrets OR as an environment variable.\")\n",
        "\n",
        "# Configure DSPy with the chosen LLM\n",
        "if llm:\n",
        "    dspy.settings.configure(lm=llm)\n",
        "    print(f\"DSPy global LM successfully configured. Active LM uses model: {llm.model if hasattr(llm, 'model') else LLM_PROVIDER_MODEL_STRING}\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: LLM was not configured. DSPy operations will fail. Please check your LLM_PROVIDER_MODEL_STRING and API key setup.\")"
      ],
      "metadata": {
        "id": "CIGuZWgBVDxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76a3267d-862f-4efb-8e63-2c36137c42d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOOGLE_API_KEY loaded from Colab secrets.\n",
            "DSPy configured to use model: gemini/gemma-3-1b-it\n",
            "DSPy global LM successfully configured. Active LM uses model: gemini/gemma-3-1b-it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the DSPy signature, which acts as a kind of recipe for your prompts. I called it `AutomatabilityRatingSignature`, and as you can see below, it provides a high level description of the expected behaviour, the full text of the request (`full_request_text`) as an input, and a numeric `rating` and an an `explanation` as ouputs. This isn't the prompt itself but rather a skeleton for the prompt. DSPy will use this, combined with the parameters we set in the previous cell, to convert our request into something the API of any model can understand.\n",
        "\n",
        "To tell DSPy to use this signature when prompting the LLM, we use it inside a Predict module (called `generate_rating` below). At this point, we could already make start firing questions to the model. For example:\n",
        "\n",
        "```\n",
        "# calling generate_rating with a question about a skill\n",
        "generate_rating(\n",
        "  full_request_text=\"Rate the automatability of the skill ~Writing~                     \\\n",
        "                     in the context of advacnemtents in generative AI                   \\\n",
        "                     in the next 5-10 years\")\n",
        "\n",
        "# returns something like\n",
        "  Rating: 4\n",
        "  Response: Writing is a complex skill that can be automated to a significant           \\\n",
        "  degree, but it still requires considerable human oversight...\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKlcIinGoWAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit the signature if you want to modify the LLM's behaviour.\n",
        "class AutomatabilityRatingSignature(dspy.Signature):\n",
        "    \"\"\"Given a specific request about a skill or work activity, provide a numerical rating for its automatability and an explanation if requested.\"\"\"\n",
        "\n",
        "    full_request_text = dspy.InputField(\n",
        "        desc=\"The complete, formatted prompt text that asks for the automatability rating and may request an explanation.\"\n",
        "        )\n",
        "    rating = dspy.OutputField(\n",
        "        desc=\"A single numerical rating on a scale of 1 to 5 (e.g., 1, 2, 3, 4, or 5).\"\n",
        "        )\n",
        "    explanation = dspy.OutputField(\n",
        "        desc=\"A brief explanation for the rating. This may be empty if the prompt did not request an explanation.\"\n",
        "        )\n",
        "\n",
        "if dspy.settings.lm is None:\n",
        "    print(\"CRITICAL ERROR: LLM not configured in dspy.settings. Please run the LLM configuration cell.\")\n",
        "else:\n",
        "    generate_rating = dspy.Predict(AutomatabilityRatingSignature)"
      ],
      "metadata": {
        "id": "f-ugP6m6eb7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 - Prompting the LLM\n",
        "\n",
        "Now that everything is set up, we can move on to actually prompting the LLM, which turns out to be surprisingly simple. In essence, for evey item in `items_to_rate` (a skill or work activity) the code iterates through all 10 `prompts`, inserts the name and description (when relevant) into the template, and calls our `generate_rating()` function with the resulting text as the prompt. It then saves the output along some identifying information in a new dictionary called `all_results`."
      ],
      "metadata": {
        "id": "-8QIzj9WxvZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flag to signal breaking out of loops when the model can't be reached.\n",
        "stop_processing_flag = False\n",
        "\n",
        "# Empty dictionary on which results will be saved\n",
        "all_results = []\n",
        "\n",
        "if 'generate_rating' not in locals():\n",
        "    print(\"Error: The 'generate_rating' DSPy predictor is not initialized. Please check step 2.\")\n",
        "else:\n",
        "    for item_info in items_to_rate:\n",
        "        item_name = item_info.get('Name')\n",
        "        item_descriptor = item_info.get('Description')\n",
        "\n",
        "        # print(f\"\\nProcessing: '{item_name}'\")     # uncomment for debugging or tracking progress\n",
        "\n",
        "        for prompt_info in prompts:\n",
        "            prompt_id = prompt_info.get('id')\n",
        "            prompt_template = prompt_info.get('template')\n",
        "\n",
        "            if not prompt_template:\n",
        "                print(f\"Skipping prompt due to missing template: {prompt_info}\")\n",
        "                continue\n",
        "\n",
        "            # Format the prompt - fill in the placeholders in the template\n",
        "            try:\n",
        "                if \"{item_descriptor}\" in prompt_template:\n",
        "                    current_full_request_text = prompt_template.format(\n",
        "                        item_name=item_name,\n",
        "                        item_descriptor=item_descriptor\n",
        "                    )\n",
        "                else:\n",
        "                    current_full_request_text = prompt_template.format(\n",
        "                        item_name=item_name\n",
        "                    )\n",
        "            except KeyError as e:\n",
        "                print(f\"KeyError during formatting for item '{item_name}' with prompt ID '{prompt_id}'. Placeholder: {e}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Call the DSPy Predictor\n",
        "            try:\n",
        "                # This sends the request to the LLM\n",
        "                prediction = generate_rating(\n",
        "                    full_request_text=current_full_request_text)\n",
        "\n",
        "                # The prediction object will have attributes corresponding to the OutputFields\n",
        "                llm_rating_raw = prediction.rating\n",
        "                llm_explanation = prediction.explanation if hasattr(prediction, 'explanation') else \"\"\n",
        "\n",
        "                result_entry = {\n",
        "                    'item_name': item_name,\n",
        "                    'prompt_id': prompt_id,\n",
        "                    'expects_explanation_flag': prompt_info.get('expects_explanation'),\n",
        "                    'llm_raw_rating_output': llm_rating_raw,\n",
        "                    'llm_explanation_output': llm_explanation,\n",
        "                }\n",
        "\n",
        "                all_results.append(result_entry)\n",
        "\n",
        "            except Exception as e:\n",
        "                error_message = str(e)\n",
        "                print(f\"ERROR during LLM call for item '{item_name}' with prompt ID '{prompt_id}': {error_message}\")\n",
        "\n",
        "                # Store error information\n",
        "                all_results.append({\n",
        "                    'item_name': item_name,\n",
        "                    'error': error_message\n",
        "                })\n",
        "\n",
        "                # Check if the error is model not found (404)\n",
        "                if \"404\" in error_message:\n",
        "                    print(\"This seems to be a critical API or model configuration error. Stopping further processing.\")\n",
        "                    stop_processing_flag = True # Signal to stop all processing\n",
        "                    break\n",
        "\n",
        "            # Rate Limiting - add a sleep timer to avoid hitting API rate limits.\n",
        "            time.sleep(2.2) # Sleep for 5 seconds (e.g., for ~12 requests per minute)\n",
        "\n",
        "        if stop_processing_flag:\n",
        "            break # Critical error found, breaking out of both loops\n",
        "\n",
        "    print(\"\\n--- Processing Complete ---\")\n",
        "    print(f\"Total results collected: {len(all_results)}\")"
      ],
      "metadata": {
        "id": "4MlxZVXbrryA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 - Store and process the results\n",
        "\n",
        "The last step is to take the dictionary containing all of our results and turn into datasets that we can work with. We'll use pandas for this, one of the most popular and powerful data libraries for python. You can learn more about it here:\n",
        "\n",
        "We create two resulting tables: `full_table_df` which contains all 990 individual ratings, and `summary_table_df` wich aggregates the responses from the different prompts by skill or work activity, calculating the mean score, the standard deviation and the range of ratings. For further exploration, it also records the LLM's explanation for the lowest and highest recorded scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "1aKTQpa51-MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df[\"llm_raw_rating_output\"] = pd.to_numeric(results_df[\"llm_raw_rating_output\"],\n",
        "                                                    errors = \"coerce\")\n",
        "\n",
        "# Filter for desired columns\n",
        "full_table_df = results_df[['item_name',\n",
        "                         'prompt_id',\n",
        "                         'llm_raw_rating_output',\n",
        "                         'llm_explanation_output']].copy()\n",
        "\n",
        "# Group by skill or work activity for aggregation\n",
        "grouped_by_item = full_table_df.groupby('item_name')\n",
        "\n",
        "# Calculate mean, std deviation, min and max\n",
        "summary_stats = grouped_by_item['llm_raw_rating_output'].agg(\n",
        "    average_rating = 'mean',\n",
        "    std_dev = 'std',\n",
        "    min = 'min',\n",
        "    max = 'max',\n",
        ").reset_index()\n",
        "\n",
        "# Create the 'range' column\n",
        "summary_stats['range'] = summary_stats.apply(\n",
        "    lambda row: f\"{int(row['min'])} - {int(row['max'])}\"\n",
        "    if pd.notna(row['min']) and pd.notna(row['max']) else \"N/A\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Extract explanation for Min and Max scores\n",
        "def get_min_max_explanations(group):\n",
        "\n",
        "    min_val = group['llm_raw_rating_output'].min()\n",
        "    max_val = group['llm_raw_rating_output'].max()\n",
        "\n",
        "    min_explanation = \"N/A\"\n",
        "    max_explanation = \"N/A\"\n",
        "\n",
        "    if pd.notna(min_val):\n",
        "        # Get the first row where parsed_rating_numeric matches min_val\n",
        "        min_row = group[group['llm_raw_rating_output'] == min_val].iloc[0]\n",
        "        min_explanation = min_row['llm_explanation_output'] if pd.notna(min_row['llm_explanation_output']) else \"N/A\"\n",
        "\n",
        "    if pd.notna(max_val):\n",
        "        # Get the first row where parsed_rating_numeric matches max_val\n",
        "        max_row = group[group['llm_raw_rating_output'] == max_val].iloc[0]\n",
        "        max_explanation = max_row['llm_explanation_output'] if pd.notna(max_row['llm_explanation_output']) else \"N/A\"\n",
        "\n",
        "    if pd.notna(min_val) and pd.notna(max_val) and min_val == max_val:\n",
        "        return f\"Min/Max ({int(min_val)}): {min_explanation}\"\n",
        "    else:\n",
        "        return f\"Min ({int(min_val) if pd.notna(min_val) else 'N/A'}): {min_explanation}\\n----------\\nMax ({int(max_val) if pd.notna(max_val) else 'N/A'}): {max_explanation}\"\n",
        "\n",
        "min_max_explanations_series = grouped_by_item.apply(get_min_max_explanations, include_groups=False)\n",
        "min_max_explanations_df = min_max_explanations_series.reset_index(name='min_max_explanations')\n",
        "\n",
        "# Merge with summary_stats\n",
        "summary_table_df = pd.merge(summary_stats, min_max_explanations_df, on='item_name')"
      ],
      "metadata": {
        "id": "rI9Z7e294bNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We save the resulting two tables into a csv files.\n",
        "# If you're on google colab, make sure to download these before\n",
        "# restarting your session or you'll lose them.\n",
        "\n",
        "full_table_df.to_csv(\"full_table_df.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "summary_table_df.to_csv(\"summary_table_df.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "NgmwNxAH75YR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}