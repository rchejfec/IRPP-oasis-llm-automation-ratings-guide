{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s3uZXCJ9VyxOR0AhrWxLuRJwN3Ts5Jh1",
      "authorship_tag": "ABX9TyOCl8K+nXAfZ6kzl7RLMOrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rchejfec/IRPP-oasis-llm-automation-ratings-guide/blob/main/IRPP_oasis_llm_automation_ratings_guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessing the automatability of OaSIS' skills and work activities using Large Language Models\n",
        "#### *A guide to replicating Oschinski & Walia (2023) for researchers and governments using DSPy*\n",
        "\n",
        "\n",
        "---\n",
        "**Author:** Ricardo Chejfec, IRPP  \n",
        "**GitHub Repository:** [IRPP-oasis-llm-automation-ratings-guide](https://github.com/rchejfec/IRPP-oasis-llm-automation-ratings-guide)  \n",
        "**Date:** June 2025\n",
        "\n",
        "---\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This notebook walks through the process of using a Large Language Model (LLM) via the DSPy framework to assess the automatability of skills and work activities from ESDC's OaSIS, based on Oschinski & Walia (2025).\n",
        "\n",
        "The easiest way to get started, especially if you're new to Python or LLMs, is to use Google Colab. It's free and relatively easy to use.\n",
        "1.  Ensure you are logged into a Google account.\n",
        "2.  Open this notebook file in Colab.\n",
        "3.  The necessary data files (`OaSIS_Guide_2023.csv` and `prompt_templates.json`) should be located in a `data/` subdirectory within the same project structure as this notebook.\n",
        "\n",
        "If you are not using Colab or a similar cloud-based developer environment, you'll need to manage your Python environment, dependencies (like DSPy, pandas, etc.), and API keys locally. While not overly complicated, this can be more involved for those new to it.\n",
        "\n",
        "**Notebook Steps:**\n",
        "1.  **Data Preparation**: Load and preprocess skills/work activities from OaSIS and prompt templates.\n",
        "2.  **LLM Configuration**: Set up the chosen LLM (e.g., Gemini, OpenAI model) with DSPy.\n",
        "3.  **Prompting and Rating**: Iterate through items and prompts, sending requests to the LLM.\n",
        "4.  **Results Processing**: Aggregate and analyze the LLM's ratings.\n"
      ],
      "metadata": {
        "id": "CZehY54s3dZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repository from GitHub to access the data files\n",
        "!git clone https://github.com/rchejfec/IRPP-oasis-llm-automation-ratings-guide.git\n",
        "\n",
        "# Navigate into the cloned repository's directory\n",
        "%cd IRPP-oasis-llm-automation-ratings-guide"
      ],
      "metadata": {
        "id": "C5xn1y_lMwhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sWCD4jIiSD76"
      },
      "outputs": [],
      "source": [
        "!pip install dspy-ai -q\n",
        "\n",
        "# If you are not running this in Google Colab, you'll need to install these libraries\n",
        "# in your local Python environment. dspy-ai includes most dependencies.\n",
        "# You can typically install them using a command like:\n",
        "# !pip install dspy-ai pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "6ipyXAAZSVIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Data Preparation\n",
        "\n",
        "**1.1. OaSIS Framework Data**\n",
        "\n",
        "We'll first get a list of all unique skills and work activities from ESDC's OaSIS framework. The necessary file (`OaSIS_Guide_2023.csv`) is included in the `Data/` subdirectory of this repository.\n",
        "The code below loads this file, extracts the relevant skills and work activities, and cleans up their descriptions.\n",
        "If you wanted to use a different data source (e.g., O*Net), you would need to adapt the loading and processing steps and ensure the final list of items to rate is a Python list of dictionaries with 'Name' and 'Description' keys in each item."
      ],
      "metadata": {
        "id": "xtAQKG_-YenW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # OaSIS Guide:\n",
        "  guide_df = pd.read_csv(\"Data/OaSIS_Guide_2023.csv\")\n",
        "\n",
        "  # Extract skills & work activities\n",
        "  items_to_rate_df = guide_df[(guide_df['Structure type'] == \"Descriptor\") &\n",
        "                          (guide_df['Category'].isin([\"Skills\", \"Work Activities\"]))]\n",
        "\n",
        "  # Convert to dict for easier wrangling\n",
        "  items_to_rate = items_to_rate_df[['Name', 'Description']].to_dict(orient=\"records\")\n",
        "\n",
        "  # Remove unecessary information that might interfere with prompts.\n",
        "  pattern_to_remove = r\"\\s*This descriptor is measured by .*? level on a scale of \\d+-\\d+\\.?\\s*$\"\n",
        "\n",
        "  for item in items_to_rate:\n",
        "    original = item.get(\"Description\")\n",
        "    cleaned = re.sub(pattern_to_remove, \"\", original, flags=re.IGNORECASE).strip()\n",
        "    item[\"Description\"] = cleaned\n",
        "\n",
        "  print(f\"Extracted list of dictionaries with {len(items_to_rate)} items with {list(items_to_rate[0].keys())} as keys.\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Failed to make a list of dictionaries from input table. Ensure you have uploaded the file and that file names are aligned.\\\n",
        "  \\nError message: {e}\")"
      ],
      "metadata": {
        "id": "BWmP_pP-esP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Prompt Templates**\n",
        "\n",
        "Next, we'll import a list of different prompt phrasings to use when querying the LLM. These phrasings come from Appendix C of the study, and are stored in `data/prompt_templates.json`. This notebook uses 10 distinct phrasings, omitting versions 7 and 9 from the original study for easier parameter handling across different models.\n",
        "\n",
        "The prompts are designed to incorporate the name and (sometimes) the description of the skills and work activities using placeholders (e.g., `{item_name}`, `{item_descriptor}`). This helps the LLM parse the question, as models vary in their ability to process longer, formally structured requests.\n",
        "\n",
        "You can edit the `prompt_templates.json` file using a text editor to remove, modify, or add new phrasings. For example, while only some of the original study's prompts included a description of the skill/activity, you could ensure all of them do by adding the `{item_descriptor}` placeholder where appropriate."
      ],
      "metadata": {
        "id": "k-V-Ueqt2BnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  with open(\"Data/prompt_templates.json\", \"r\") as f:\n",
        "    prompts = json.load(f)\n",
        "    print(f\"Imported {len(prompts)} prompts.\")\n",
        "except Exception as e:\n",
        "  print(f\"Failed to import list of prompts. Ensure you have uploaded the file and that file names are aligned.\\\n",
        "  \\nError message: {e}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XDlMViM4mn4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 - Configure LLM and DSPy\n",
        "\n",
        "The next step is to initialize and configure the LLM and define how we'll interact with it using the DSPy framework.\n",
        "\n",
        "This notebook is configured by default to use Google's `gemini/gemma-3-1b-it` model. **This specific model was chosen for demonstration and testing purposes in this notebook, primarily because it is relatively fast, available with free tiers, and comes with generous API rate limits, making it suitable for running the ~1,000 requests generated by this script without incurring significant costs.** However, DSPy's flexibility allows you to easily use other models like OpenAI's GPT series, Llama models via Groq, or more powerful Gemini models from Google. You'll typically need to find the correct model identifier string for your chosen provider (e.g., `openai/gpt-3.5-turbo`, `google/gemini-1.5-pro-latest`). For more robust research, consider a larger, more capable model, keeping in mind potential costs and rate limits.\n",
        "\n",
        "**Crucially, you must handle your API key securely:**\n",
        "* Do not share or accidentally publish your API key.\n",
        "* The code below is set up to use Google Colab's \"Secrets\" feature (accessed via the ðŸ”‘ key icon on the left navigation bar). Add your API key there, give it a name (the code defaults to looking for `GOOGLE_API_KEY`), and ensure notebook access is enabled.\n",
        "* Remember to update the `API_KEY_NAME` variable in the code if your secret has a different name.\n",
        "* If running locally, manage your API key using environment variables or other secure methods. For guidance on local key management, you can consult resources like [this article on managing API keys in Python projects](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
      ],
      "metadata": {
        "id": "knrOJaVJ6nzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the name and version of the LLM (Defaults to Gemma)\n",
        "LLM_PROVIDER_MODEL_STRING = \"gemini/gemma-3-1b-it\"\n",
        "\n",
        "# Initialize variables\n",
        "API_KEY = None\n",
        "llm = None\n",
        "API_KEY_NAME = \"GOOGLE_API_KEY\"   # update with your own name if different\n",
        "\n",
        "# Load API Key\n",
        "# If running in Google Colab, set your API key in the \"Secrets\" tab.\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "  API_KEY = userdata.get(API_KEY_NAME)\n",
        "  if API_KEY:\n",
        "      print(f\"{API_KEY_NAME} loaded from Colab secrets.\")\n",
        "except ImportError:\n",
        "  print(\"Could not import Colab userdata\")\n",
        "  pass\n",
        "\n",
        "# If not using Google Colab, implement your key loading here.\n",
        "# A common method is using environment variables:\n",
        "# import os\n",
        "# API_KEY = os.getenv(\"YOUR_API_KEY_ENVIRONMENT_VARIABLE_NAME\")\n",
        "\n",
        "# Set model parameters\n",
        "if API_KEY:\n",
        "  try:\n",
        "      model_config = {\n",
        "          \"temperature\": 0.2,     # change the desired temperature\n",
        "          \"max_tokens\": 300}      # change the desired number of max tokens\n",
        "      llm = dspy.LM(\n",
        "          model=LLM_PROVIDER_MODEL_STRING,\n",
        "          api_key=API_KEY,\n",
        "          temperature=model_config[\"temperature\"],\n",
        "          max_tokens= model_config[\"max_tokens\"],\n",
        "          timeout=60,             # number of seconds before it retries. lengthen if your connection is spotty\n",
        "          num_retries=3           # number of times it should retry after timing out\n",
        "          )\n",
        "\n",
        "      print(f\"DSPy configured to use model: {LLM_PROVIDER_MODEL_STRING}\")\n",
        "  except Exception as e:\n",
        "      print(f\"ERROR: Could not configure DSPy with {LLM_PROVIDER_MODEL_STRING}. Exception: {e}\")\n",
        "      llm = None\n",
        "else:\n",
        "  print(f\"ERROR: {API_KEY_NAME} not found. Please set it in Colab secrets OR as an environment variable.\")\n",
        "\n",
        "# Configure DSPy with the chosen LLM\n",
        "if llm:\n",
        "    dspy.settings.configure(lm=llm)\n",
        "    print(f\"DSPy global LM successfully configured. Active LM uses model: {llm.model if hasattr(llm, 'model') else LLM_PROVIDER_MODEL_STRING}\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: LLM was not configured. DSPy operations will fail. Please check your LLM_PROVIDER_MODEL_STRING and API key setup.\")"
      ],
      "metadata": {
        "id": "CIGuZWgBVDxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the DSPy **Signature**, which acts as a blueprint for our LLM interactions. I've named it `AutomatabilityRatingSignature`. As you'll see in the code below, it provides a high-level description of the expected behavior. It also specifies:\n",
        "* An **input field**: `full_request_text` (the complete, formatted prompt asking for the automatability rating).\n",
        "* **Output fields**: A numeric `rating`.\n",
        "\n",
        "Since the study's prompts vary on whether they ask for an explanation, we create a second signature `AutomatabilityRatingAndExplanationSignature`, identical to the first one, but including a second output field `explanation`.\n",
        "\n",
        "This Signature isn't the prompt itself but rather a structured template. DSPy uses this, combined with the LLM configuration from the previous cell, to format requests and parse responses from any compatible model API.\n",
        "\n",
        "To instruct DSPy to use this Signature for prompting the LLM, we wrap it in a `dspy.Predict` module, which we'll call `generate_rating`. At this point, we could already start sending requests to the model. For example:\n",
        "\n",
        "```\n",
        "# Example of calling generate_rating with a question about a skill:\n",
        "# prediction = generate_rating(\n",
        "#     full_request_text=\"Rate the automatability of the skill ~Writing~ in the context of advancements  \\\n",
        "#                        in generative AI in the next 5-10 years. Provide a rating from 1-5 and a brief \\\n",
        "#                        explanation.\")\n",
        "# print(f\"Rating: {prediction.rating}\")\n",
        "# print(f\"Explanation: {prediction.explanation}\")\n",
        "\n",
        "# This might return something like:\n",
        "#  Rating:'4',\n",
        "#  Explanation:'Writing is a complex skill that can be automated to a significant degree by generative  \\\n",
        "#               AI for tasks like ...'\n",
        "```\n",
        "*(Note: The actual output structure is a `Prediction` object with attributes for each output field.)*"
      ],
      "metadata": {
        "id": "AKlcIinGoWAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit the signature if you want to modify the LLM's behaviour.\n",
        "class AutomatabilityRatingSignature(dspy.Signature):\n",
        "    \"\"\"Given a specific request about a skill or work activity, provide a numerical rating.\"\"\"\n",
        "\n",
        "    full_request_text = dspy.InputField(\n",
        "        desc=\"The complete, formatted prompt text.\")\n",
        "\n",
        "    rating = dspy.OutputField(\n",
        "        desc=\"A single numerical rating on a scale of 1 (low) to 5 (high) (e.g., 1, 2, 3, 4, 5).\")\n",
        "\n",
        "class AutomatabilityRatingAndExplanationSignature(dspy.Signature):\n",
        "    \"\"\"Given a specific request about a skill or work activity, provide a numerical rating.\"\"\"\n",
        "\n",
        "    full_request_text = dspy.InputField(\n",
        "        desc=\"The complete, formatted prompt text.\")\n",
        "\n",
        "    rating = dspy.OutputField(\n",
        "        desc=\"A single numerical rating on a scale of 1 (low) to 5 (high) (e.g., 1, 2, 3, 4, 5).\")\n",
        "\n",
        "    explanation = dspy.OutputField(\n",
        "        desc=\"A brief, concise explanation (1-3 sentences) for the rating.\")\n",
        "\n",
        "if dspy.settings.lm is None:\n",
        "    print(\"CRITICAL ERROR: LLM not configured in dspy.settings. Please run the LLM configuration cell.\")\n",
        "else:\n",
        "    generate_rating = dspy.Predict(AutomatabilityRatingSignature)\n",
        "    generate_rating_explanation = dspy.Predict(AutomatabilityRatingAndExplanationSignature)\n",
        "    print(\"Prediction modules initiliazed\")"
      ],
      "metadata": {
        "id": "f-ugP6m6eb7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 - Prompting the LLM\n",
        "\n",
        "Now that everything is set up, we can move on to actually prompting the LLM, which turns out to be surprisingly simple.\n",
        "\n",
        "In essence, for every item in our `items_to_rate` list (each representing a skill or work activity), the code iterates through all 10 `prompts` or phrasings. It inserts the item's name and description (when relevant) into the prompt template and then calls our `generate_rating()` function, sending the resulting text as the prompt to the LLM. The LLM's output (rating and explanation) is then saved, along with some identifying information, into a list of dictionaries called `all_results`."
      ],
      "metadata": {
        "id": "-8QIzj9WxvZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flag to signal breaking out of loops when the model can't be reached.\n",
        "stop_processing_flag = False\n",
        "\n",
        "# Empty list on which results will be saved\n",
        "all_results = []\n",
        "\n",
        "if 'generate_rating' not in locals():\n",
        "    print(\"Error: The 'generate_rating' DSPy predictor is not initialized. Please check step 2.\")\n",
        "else:\n",
        "    for item_info in items_to_rate:\n",
        "        item_name = item_info.get('Name')\n",
        "        item_descriptor = item_info.get('Description')\n",
        "        # print(f\"\\nProcessing: '{item_name}'\")     # uncomment for debugging or tracking progress\n",
        "\n",
        "        for prompt_info in prompts:\n",
        "            prompt_id = prompt_info.get('id')\n",
        "            prompt_template = prompt_info.get('template')\n",
        "            prompt_explanation = prompt_info.get('expects_explanation')\n",
        "\n",
        "            if not prompt_template:\n",
        "                print(f\"Skipping prompt due to missing template: {prompt_info}\")\n",
        "                continue\n",
        "\n",
        "            # Format the prompt - fill in the placeholders in the template\n",
        "            try:\n",
        "                if \"{item_descriptor}\" in prompt_template:\n",
        "                    current_full_request_text = prompt_template.format(\n",
        "                        item_name=item_name,\n",
        "                        item_descriptor=item_descriptor\n",
        "                    )\n",
        "                else:\n",
        "                    current_full_request_text = prompt_template.format(\n",
        "                        item_name=item_name\n",
        "                    )\n",
        "            except KeyError as e:\n",
        "                print(f\"KeyError during formatting for item '{item_name}' with prompt ID '{prompt_id}'. Placeholder: {e}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Call the DSPy Predictor\n",
        "            try:\n",
        "                # This sends the request to the LLM\n",
        "                # Depending on whether the prompt calls for an explanation\n",
        "                # we use generate_rating() or generate_rating_explanation()\n",
        "                if prompt_explanation:\n",
        "                  prediction = generate_rating_explanation(\n",
        "                      full_request_text=current_full_request_text)\n",
        "                else:\n",
        "                  prediction = generate_rating(\n",
        "                      full_request_text=current_full_request_text)\n",
        "\n",
        "                # The prediction object will have attributes corresponding to the OutputFields\n",
        "                llm_rating_raw = prediction.rating\n",
        "                llm_explanation = prediction.explanation if hasattr(prediction, 'explanation') else \"\"\n",
        "\n",
        "                result_entry = {\n",
        "                    'item_name': item_name,\n",
        "                    'prompt_id': prompt_id,\n",
        "                    'expects_explanation_flag': prompt_explanation,\n",
        "                    'llm_raw_rating_output': llm_rating_raw,\n",
        "                    'llm_explanation_output': llm_explanation,\n",
        "                }\n",
        "\n",
        "                all_results.append(result_entry)\n",
        "\n",
        "            except Exception as e:\n",
        "                error_message = str(e)\n",
        "                print(f\"ERROR during LLM call for item '{item_name}' with prompt ID '{prompt_id}': {error_message}\")\n",
        "\n",
        "                # Store error information\n",
        "                all_results.append({\n",
        "                    'item_name': item_name,\n",
        "                    'error': error_message\n",
        "                })\n",
        "\n",
        "                # Check if the error is model not found (404)\n",
        "                if \"404\" in error_message:\n",
        "                    print(\"This seems to be a critical API or model configuration error. Stopping further processing.\")\n",
        "                    stop_processing_flag = True # Signal to stop all processing\n",
        "                    break\n",
        "\n",
        "            # Rate Limiting - add a sleep timer to avoid hitting API rate limits.\n",
        "            time.sleep(2.2) # Sleep for 5 seconds (e.g., for ~12 requests per minute)\n",
        "\n",
        "        if stop_processing_flag:\n",
        "            break # Critical error found, breaking out of both loops\n",
        "\n",
        "    print(\"\\n Processing Complete\")\n",
        "    print(f\"Total results collected: {len(all_results)}\")"
      ],
      "metadata": {
        "id": "4MlxZVXbrryA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 - Store and process the results\n",
        "\n",
        "The final step is to take the `all_results` list (which contains all the raw outputs from the LLM) and transform it into structured datasets that we can work with more easily. We'll use pandas for this, one of the most popular and powerful data manipulation libraries for Python. You can learn more about pandas [here](https://pandas.pydata.org/docs/).\n",
        "\n",
        "We create two main tables (Pandas DataFrames):\n",
        "1.  `full_table_df`: This table contains all 990 (99 items * 10 prompts) individual ratings, including the raw rating output and any explanation provided by the LLM for each specific prompt.\n",
        "2.  `summary_table_df`: This table aggregates the responses from the different prompts for each unique skill or work activity. It calculates the mean score, standard deviation, and the range (minimum and maximum) of ratings. For further exploration, it also records the LLM's explanation corresponding to the lowest and highest recorded scores for which explanations were required."
      ],
      "metadata": {
        "id": "1aKTQpa51-MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df[\"llm_raw_rating_output\"] = pd.to_numeric(results_df[\"llm_raw_rating_output\"],\n",
        "                                                    errors = \"coerce\")\n",
        "\n",
        "# Filter for desired columns\n",
        "full_table_df = results_df[['item_name',\n",
        "                         'prompt_id',\n",
        "                         'expects_explanation_flag',\n",
        "                         'llm_raw_rating_output',\n",
        "                         'llm_explanation_output']].copy()\n",
        "\n",
        "# Group by skill or work activity for aggregation\n",
        "grouped_by_item = full_table_df.groupby('item_name')\n",
        "\n",
        "# Calculate mean, std deviation, min and max\n",
        "summary_stats = grouped_by_item['llm_raw_rating_output'].agg(\n",
        "    average_rating = 'mean',\n",
        "    std_dev = 'std',\n",
        "    min = 'min',\n",
        "    max = 'max',\n",
        ").reset_index()\n",
        "\n",
        "# Create the 'range' column\n",
        "summary_stats['range'] = summary_stats.apply(\n",
        "    lambda row: f\"{int(row['min'])} - {int(row['max'])}\"\n",
        "    if pd.notna(row['min']) and pd.notna(row['max']) else \"N/A\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "\n",
        "def get_min_max_explanations(group):\n",
        "    \"\"\"\n",
        "    Finds the min and max scores from the subset of prompts that expected an explanation,\n",
        "    and returns those scores along with their corresponding explanations.\n",
        "    \"\"\"\n",
        "    # Filter for prompts that expected an explanation\n",
        "    explanation_group = group[group['expects_explanation_flag'] == True]\n",
        "\n",
        "    if explanation_group.empty:\n",
        "        return \"No explanations were requested for this item.\"\n",
        "\n",
        "    # Find the min and max ratings within filtered subset\n",
        "    min_val = explanation_group['llm_raw_rating_output'].min()\n",
        "    max_val = explanation_group['llm_raw_rating_output'].max()\n",
        "\n",
        "    min_explanation_text = \"N/A\"\n",
        "    max_explanation_text = \"N/A\"\n",
        "\n",
        "    # Process the MINIMUM score's explanation\n",
        "    if pd.notna(min_val):\n",
        "        min_row = explanation_group[explanation_group['llm_raw_rating_output'] == min_val].iloc[0]\n",
        "        explanation = min_row['llm_explanation_output']\n",
        "        if isinstance(explanation, str) and explanation.strip():\n",
        "            min_explanation_text = f\"(Score: {int(min_val)}) {explanation.strip()}\"\n",
        "\n",
        "    # Process the MAXIMUM score's explanation\n",
        "    if pd.notna(max_val):\n",
        "        max_row = explanation_group[explanation_group['llm_raw_rating_output'] == max_val].iloc[0]\n",
        "        explanation = max_row['llm_explanation_output']\n",
        "        if isinstance(explanation, str) and explanation.strip():\n",
        "            max_explanation_text = f\"(Score: {int(max_val)}) {explanation.strip()}\"\n",
        "\n",
        "    # Format the final combined string\n",
        "    if min_val == max_val and pd.notna(min_val):\n",
        "        return min_explanation_text\n",
        "    else:\n",
        "        return f\"Min: {min_explanation_text}\\n----------\\nMax: {max_explanation_text}\"\n",
        "\n",
        "min_max_explanations_series = grouped_by_item.apply(get_min_max_explanations, include_groups=False)\n",
        "min_max_explanations_df = min_max_explanations_series.reset_index(name='min_max_explanations')\n",
        "\n",
        "# Merge with summary_stats\n",
        "summary_table_df = pd.merge(summary_stats, min_max_explanations_df, on='item_name')\n",
        "\n",
        "print(\"Processed results into dataframes\")"
      ],
      "metadata": {
        "id": "rI9Z7e294bNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the two DataFrames (`full_table_df` and `summary_table_df`) are saved as CSV files. If you are running this in Google Colab, these files will appear in the session's file browser (usually accessible via a folder icon on the left sidebar), and you can download them from there for your records or further analysis."
      ],
      "metadata": {
        "id": "gAeX5NuhFEmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output directory\n",
        "output_dir = \"Results\"\n",
        "\n",
        "# Create the output directory if it doesn't already exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    print(f\"Created directory: {output_dir}\")\n",
        "\n",
        "# Extract the model name and get current date\n",
        "model_name_for_filename = LLM_PROVIDER_MODEL_STRING.split('/')[-1]\n",
        "current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "# Create the full file paths, including the new directory\n",
        "full_results_filepath = os.path.join(output_dir, f\"full_results_{model_name_for_filename}_{current_date}.csv\")\n",
        "summary_filepath = os.path.join(output_dir, f\"summary_results_{model_name_for_filename}_{current_date}.csv\")\n",
        "\n",
        "print(f\"Saving full results to: {full_results_filepath}\")\n",
        "print(f\"Saving summary to: {summary_filepath}\")\n",
        "\n",
        "# Save the resulting two tables into the specified directory\n",
        "# If you're on Google Colab, make sure to download the files from the 'results'\n",
        "# folder before restarting your session or you'll lose them.\n",
        "\n",
        "full_table_df.to_csv(full_results_filepath, index=False, encoding='utf-8-sig')\n",
        "summary_table_df.to_csv(summary_filepath, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\nFiles saved successfully\")"
      ],
      "metadata": {
        "id": "NgmwNxAH75YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "\n",
        "The results generated by this notebook should be viewed as an informative starting point rather than a definitive prediction. It is crucial to be critical of the outputs, especially when using lighter models for testing, as was done here for demonstration. While DSPy provides a more reliable prompting structure, choices regarding the Signature design, the specific LLM used, and its configuration parameters (temperature, max_tokens, etc.) can all influence the final ratings. More research is required to validate these methods and understand the optimal approaches for this type of assessment.\n",
        "\n",
        "This project opens the door to many exciting avenues for further testing and more sophisticated research. For example:\n",
        "\n",
        "* Model Specialization: Could a base model be fine-tuned on literature about automation and skills for more accurate and consistent results?\n",
        "* Context-Rich Ratings: Could the ratings be improved by providing the LLM with additional, real-time information? For instance, using a Retrieval-Augmented Generation (RAG) approach within DSPy to give the model access to specific job descriptions or recent labor market reports before it makes a rating.\n",
        "* Regional Analysis: Could the estimates be made more granular by incorporating region-specific economic data or job information into the prompts?\n",
        "* Impact of Phrasing: What is the quantifiable impact of different prompt phrasings on the final ratings, and can this effect be predicted or controlled?"
      ],
      "metadata": {
        "id": "fRx4OjGqycD7"
      }
    }
  ]
}